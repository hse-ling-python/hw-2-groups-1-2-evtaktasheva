{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ2 часть 1\n",
    "### Такташева Катя, БКЛ182\n",
    "\n",
    "Импортируем нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1: Текст книги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем текст книги в переменную text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book.txt', 'r', encoding='utf-8') as fid:\n",
    "    text = fid.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2: MyStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lemmas_ms = m.analyze(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем полученные данные в формат .json и запишем в файл 'lemmas_mystem.json':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmas_mystem.json', 'w', encoding='utf-8') as fid:\n",
    "    json.dump(lemmas_ms, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3: PyMorphy\n",
    "Импортируем нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем текст в токены для анализа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "lemmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for token in tokens:\n",
    "    lemma = morph.parse(token)\n",
    "    lemmas.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Вытащим\" из полученных данных информацию о словоформе, а также всех вариантах разбора этой словоформы, и добавим в словарь лексему, часть речи и число каждого из них.\n",
    "\n",
    "**P.S.:** Далее при создании словаря наткнулась на проблему, что некоторые варианты парсинга одного слова могут различаться буквой е/ё (например, один из вариантов разбора слова 'весны' - 'вёсны'), поэтому в ключе заменим все 'ё' на 'е'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict()\n",
    "lemmas_pm = []\n",
    "for i in range(len(lemmas)):\n",
    "    dic = dict()\n",
    "    dic[lemmas[i][0].word.replace('ё', 'е')] = []\n",
    "    for wordform in lemmas[i]:\n",
    "        dic[wordform.word.replace('ё', 'е')].append({\n",
    "            'normal_form': wordform.normal_form,\n",
    "            'pos': wordform.tag.POS,\n",
    "            'number': wordform.tag.number\n",
    "            })\n",
    "    lemmas_pm.append(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем полученные данные в формат .json и запишем в файл 'lemmas_pymorphy.json':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmas_pymorphy.json', 'w', encoding='utf-8') as fid:\n",
    "    json.dump(lemmas_pm, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.1: Части речи\n",
    "**NB:** Для анализа будут использованы данные, распарсенные Mystem, потому что они более точные. Стоп-слова не убираем.\n",
    "\n",
    "Импортируем Counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные из формата json в объект питона:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "parced_data = []\n",
    "for line in open('lemmas_mystem.json', encoding='utf-8'):\n",
    "    parced_data.extend(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество каждой из частей речи в тексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "for line in parced_data:\n",
    "    if 'analysis' in line.keys():\n",
    "        if line['analysis'] != []:\n",
    "            pos = line['analysis'][0]['gr']\n",
    "            pos = pos.replace('=', ',').split(',')[0]\n",
    "            pos_list.append(pos)\n",
    "counter_pos = Counter(pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим строку frequency, в которую запишем все нужные для вывода даные: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 'POS:\\tДоля слов:\\n'\n",
    "for pos, count in counter_pos.items():\n",
    "    percent = count / len(pos_list)\n",
    "    frequency += pos + '\\t' + str(percent) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Частотность слов по частям речи:', frequency, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.2: Топ-20 глаголов и наречий\n",
    "Пройдемся по словарю и добавим в списки глаголов и наречий лексемы. Для деления заменим знак =, используемый для отсечения части речи и ее неизменяемых признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_list = []\n",
    "verb_list = []\n",
    "for line in parced_data:\n",
    "    if 'analysis' in line.keys():\n",
    "        if line['analysis'] != []:\n",
    "            pos = line['analysis'][0]['gr'].replace('=', ',').split(',')[0]\n",
    "            if pos == 'V':\n",
    "                verb_list.append(line['analysis'][0]['lex'])\n",
    "            elif pos == 'ADV':\n",
    "                adv_list.append(line['analysis'][0]['lex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим строку avd_top, в которую запишем топ-20 наречий, встречающихся в тексте: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_top = '№:\\tЛексема:\\tКол-во употреблений:\\n'\n",
    "i = 1\n",
    "for word, count in Counter(adv_list).most_common(20):\n",
    "    adv_top += str(i) + '\\t' + word + '\\t\\t' + str(count) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для глаголов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_top = '№:\\tЛексема:\\tКол-во употреблений:\\n'\n",
    "i = 1\n",
    "for word, count in Counter(verb_list).most_common(20):\n",
    "    verb_top += str(i) + '\\t' + word + '\\t\\t' + str(count) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Топ-20 глаголов:', verb_top, sep='\\n\\n', end='\\n')\n",
    "print('Топ-20 наречий:', adv_top, sep='\\n\\n', end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5: Топ-20 биграмм и триграмм\n",
    "Создадим список лемм из текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lemmas = []\n",
    "for word in parced_data:\n",
    "    if 'analysis' in word.keys():\n",
    "        if word['analysis'] != []:\n",
    "            lemma = word['analysis'][0]['lex']\n",
    "            words_lemmas.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все 2- и 3-граммы, объединим их в одном списке и создадим частотный словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigr = list(nltk.bigrams(words_lemmas))\n",
    "trigr = list(nltk.trigrams(words_lemmas))\n",
    "bigrams_counter = Counter(bigr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем топ-25 2-грамм в строку bigrams_top. Аналогично с 3-граммами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_top = '№:\\tКол-во:\\tn-грамма:\\n'\n",
    "i = 1\n",
    "for gram, count in bigrams_counter.most_common(25):\n",
    "    bigrams_top += str(i) + '\\t' + str(count) + '\\t' + ' '.join(gram) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_top = '№:\\tКол-во:\\tn-грамма:\\n'\n",
    "i = 1\n",
    "for gram, count in Counter(trigr).most_common(25):\n",
    "    trigrams_top += str(i) + '\\t' + str(count) + '\\t' + ' '.join(gram) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Самые частотные 2-граммы:', bigrams_top, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При анализе 2-грамм можно заметить, что чамыми частотными являются имена героев произведения, а также фразы в принципе частотные для русского языка связки, типа \"то есть\", \"так и\", \"и не\" и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Самые частотные 3-граммы:', trigrams_top, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим схожие результаты: в топе находятся имя главного героя и, соответсвенно, его самые частотные действия: спрашивать, быть, увидеть и т.д., а также частые русские триграммы: время от времени, во всяком случае, в то что, и так далее. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

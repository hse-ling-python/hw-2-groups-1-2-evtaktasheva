{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ2 часть 1\n",
    "### Такташева Катя, БКЛ182\n",
    "\n",
    "По какой-то причине после запуска проверки Jupyter в один момент просто перестал выводить хоть что-либо в этой тетрадке... При копии тетрадки и запуске без проверки на пеп все нормально работает, поэтому закомментила, может это у меня что на ноуте. До этого все нормально прошло проверку и работало отлично..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1: Текст книги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем текст книги в переменную text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book.txt', 'r', encoding='utf-8') as fid:\n",
    "    text = fid.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2: MyStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 537 ms, total: 1.6 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmas_ms = m.analyze(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем полученные данные в формат .json и запишем в файл 'lemmas_mystem.json':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmas_mystem.json', 'w', encoding='utf-8') as fid:\n",
    "    json.dump(lemmas_ms, fid, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3: PyMorphy\n",
    "Импортируем нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем текст в токены для анализа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "lemmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.89 s, sys: 75.8 ms, total: 4.96 s\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for token in tokens:\n",
    "    lemma = morph.parse(token)\n",
    "    lemmas.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Вытащим\" из полученных данных информацию о словоформе, а также всех вариантах разбора этой словоформы, и добавим в словарь лексему, часть речи и число каждого из них.\n",
    "\n",
    "**P.S.:** Далее при создании словаря наткнулась на проблему, что некоторые варианты парсинга одного слова могут различаться буквой е/ё (например, один из вариантов разбора слова 'весны' - 'вёсны'), поэтому в ключе заменим все 'ё' на 'е'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict()\n",
    "lemmas_pm = []\n",
    "for i in range(len(lemmas)):\n",
    "    dic = dict()\n",
    "    dic[lemmas[i][0].word.replace('ё', 'е')] = []\n",
    "    for wordform in lemmas[i]:\n",
    "        dic[wordform.word.replace('ё', 'е')].append({\n",
    "            'normal_form': wordform.normal_form,\n",
    "            'pos': wordform.tag.POS,\n",
    "            'number': wordform.tag.number\n",
    "        })\n",
    "    lemmas_pm.append(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем полученные данные в формат .json и запишем в файл 'lemmas_pymorphy.json':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmas_pymorphy.json', 'w', encoding='utf-8') as fid:\n",
    "    json.dump(lemmas_pm, fid, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.1: Части речи\n",
    "**NB:** Для анализа будут использованы данные, распарсенные Mystem, потому что они более точные. Стоп-слова не убираем.\n",
    "\n",
    "Импортируем Counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные из формата json в объект питона:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parced_data = []\n",
    "for line in open('lemmas_mystem.json', encoding='utf-8'):\n",
    "    parced_data.extend(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество каждой из частей речи в тексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "for line in parced_data:\n",
    "    if 'analysis' in line.keys():\n",
    "        if line['analysis'] != []:\n",
    "            pos = line['analysis'][0]['gr']\n",
    "            pos = pos.replace('=', ',').split(',')[0]\n",
    "            pos_list.append(pos)\n",
    "counter_pos = Counter(pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим строку frequency, в которую запишем все нужные для вывода даные: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 'POS:\\tДоля слов:\\n'\n",
    "for pos, count in counter_pos.items():\n",
    "    percent = count / len(pos_list)\n",
    "    frequency += pos + '\\t' + str(percent) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частотность слов по частям речи:\n",
      "\n",
      "POS:\tДоля слов:\n",
      "S\t0.2899892809388572\n",
      "ANUM\t0.0038013990032378193\n",
      "PART\t0.05455670604356138\n",
      "CONJ\t0.08207264650304444\n",
      "SPRO\t0.06148541876167218\n",
      "V\t0.17737283546793675\n",
      "PR\t0.10734531952747726\n",
      "A\t0.09134408186268551\n",
      "ADVPRO\t0.020145204601460888\n",
      "APRO\t0.03876542937022753\n",
      "ADV\t0.06354082636226006\n",
      "NUM\t0.007801708419435757\n",
      "INTJ\t0.0017570419811477132\n",
      "COM\t2.2101156995568717e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Частотность слов по частям речи:', frequency, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.2: Топ-20 глаголов и наречий\n",
    "Пройдемся по словарю и добавим в списки глаголов и наречий лексемы. Для деления заменим знак =, используемый для отсечения части речи и ее неизменяемых признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_list = []\n",
    "verb_list = []\n",
    "for line in parced_data:\n",
    "    if 'analysis' in line.keys():\n",
    "        if line['analysis'] != []:\n",
    "            pos = line['analysis'][0]['gr'].replace('=', ',').split(',')[0]\n",
    "            if pos == 'V':\n",
    "                verb_list.append(line['analysis'][0]['lex'])\n",
    "            elif pos == 'ADV':\n",
    "                adv_list.append(line['analysis'][0]['lex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим строку avd_top, в которую запишем топ-20 наречий, встречающихся в тексте: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_top = '№:\\tЛексема:\\tКол-во употреблений:\\n'\n",
    "i = 1\n",
    "for word, count in Counter(adv_list).most_common(20):\n",
    "    adv_top += str(i) + '\\t' + word + '\\t\\t' + str(count) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для глаголов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_top = '№:\\tЛексема:\\tКол-во употреблений:\\n'\n",
    "i = 1\n",
    "for word, count in Counter(verb_list).most_common(20):\n",
    "    verb_top += str(i) + '\\t' + word + '\\t\\t' + str(count) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 глаголов:\n",
      "\n",
      "№:\tЛексема:\tКол-во употреблений:\n",
      "1\tбыть\t\t1173\n",
      "2\tстановиться\t\t224\n",
      "3\tзнать\t\t217\n",
      "4\tсказать\t\t200\n",
      "5\tмочь\t\t177\n",
      "6\tговорить\t\t127\n",
      "7\tдавать\t\t127\n",
      "8\tпонимать\t\t120\n",
      "9\tхотеть\t\t110\n",
      "10\tвидеть\t\t96\n",
      "11\tспрашивать\t\t87\n",
      "12\tвыходить\t\t86\n",
      "13\tстоять\t\t84\n",
      "14\tдумать\t\t80\n",
      "15\tоставаться\t\t77\n",
      "16\tоказываться\t\t70\n",
      "17\tидти\t\t69\n",
      "18\tсидеть\t\t69\n",
      "19\tотвечать\t\t67\n",
      "20\tвзять\t\t66\n",
      "\n",
      "Топ-20 наречий:\n",
      "\n",
      "№:\tЛексема:\tКол-во употреблений:\n",
      "1\tеще\t\t333\n",
      "2\tуже\t\t150\n",
      "3\tтеперь\t\t134\n",
      "4\tмного\t\t104\n",
      "5\tочень\t\t100\n",
      "6\tможно\t\t97\n",
      "7\tхорошо\t\t96\n",
      "8\tсейчас\t\t87\n",
      "9\tвдруг\t\t74\n",
      "10\tнадо\t\t73\n",
      "11\tконечно\t\t69\n",
      "12\tсразу\t\t65\n",
      "13\tболее\t\t61\n",
      "14\tсовсем\t\t60\n",
      "15\tскоро\t\t58\n",
      "16\tдалеко\t\t57\n",
      "17\tснова\t\t54\n",
      "18\tнужно\t\t47\n",
      "19\tназад\t\t46\n",
      "20\tсначала\t\t46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Топ-20 глаголов:', verb_top, sep='\\n\\n', end='\\n')\n",
    "print('Топ-20 наречий:', adv_top, sep='\\n\\n', end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5: Топ-20 биграмм и триграмм\n",
    "Создадим список лемм из текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lemmas = []\n",
    "for word in parced_data:\n",
    "    if 'analysis' in word.keys():\n",
    "        if word['analysis'] != []:\n",
    "            lemma = word['analysis'][0]['lex']\n",
    "            words_lemmas.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все 2- и 3-граммы, объединим их в одном списке и создадим частотный словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigr = list(nltk.bigrams(words_lemmas))\n",
    "trigr = list(nltk.trigrams(words_lemmas))\n",
    "bigrams_counter = Counter(bigr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем топ-25 2-грамм в строку bigrams_top. Аналогично с 3-граммами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_top = '№:\\tКол-во:\\t2-грамма:\\n'\n",
    "i = 1\n",
    "for gram, count in bigrams_counter.most_common(25):\n",
    "    bigrams_top += str(i) + '\\t' + str(count) + '\\t' + ' '.join(gram) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_top = '№:\\tКол-во:\\t3-грамма:\\n'\n",
    "i = 1\n",
    "for gram, count in Counter(trigr).most_common(25):\n",
    "    trigrams_top += str(i) + '\\t' + str(count) + '\\t' + ' '.join(gram) + '\\n'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые частотные 2-граммы:\n",
      "\n",
      "№:\tКол-во:\t2-грамма:\n",
      "1\t244\tфон дорн\n",
      "2\t116\tи не\n",
      "3\t106\tто что\n",
      "4\t84\tи в\n",
      "5\t74\tпотому что\n",
      "6\t73\tя не\n",
      "7\t70\tне быть\n",
      "8\t61\tне знать\n",
      "9\t60\tда и\n",
      "10\t55\tне становиться\n",
      "11\t55\tв этот\n",
      "12\t54\tзнать что\n",
      "13\t54\tбыть не\n",
      "14\t53\tу я\n",
      "15\t51\tчто это\n",
      "16\t50\tвсе равный\n",
      "17\t47\tне мочь\n",
      "18\t47\tи быть\n",
      "19\t47\tу он\n",
      "20\t46\tчто я\n",
      "21\t45\tон не\n",
      "22\t45\tмы с\n",
      "23\t44\tвы не\n",
      "24\t43\tв самый\n",
      "25\t43\tиосиф гурамович\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Самые частотные 2-граммы:', bigrams_top, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При анализе 2-грамм можно заметить, что чамыми частотными являются имена героев произведения, а также сочетания в принципе частотные для русского языка - \"не знать\", \"не быть\", \"не мочь\" - и связки типа \"потому что\", \"то что\", \"и не\" и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые частотные 3-граммы:\n",
      "\n",
      "№:\tКол-во:\t3-грамма:\n",
      "1\t28\tв самый дело\n",
      "2\t26\tмы с вы\n",
      "3\t23\tкапитан фон дорн\n",
      "4\t21\tи в самый\n",
      "5\t19\tкорнелиус фон дорн\n",
      "6\t16\tо то что\n",
      "7\t16\tк то же\n",
      "8\t13\tв то что\n",
      "9\t12\tне знать что\n",
      "10\t12\tперед то как\n",
      "11\t12\tв тот же\n",
      "12\t11\tэто и быть\n",
      "13\t11\tтак и не\n",
      "14\t11\tмы с ты\n",
      "15\t11\tбиблиотека иван грозный\n",
      "16\t10\tв тот пора\n",
      "17\t10\tзнать что я\n",
      "18\t10\tи без то\n",
      "19\t10\tна самый дело\n",
      "20\t10\tа то и\n",
      "21\t9\tфон дорн и\n",
      "22\t9\tв конец конец\n",
      "23\t9\tни за что\n",
      "24\t9\tу он быть\n",
      "25\t9\tу я быть\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Самые частотные 3-граммы:', trigrams_top, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим схожие результаты: в топе находятся имя главного героя, а также частые русские триграммы: \"в самом деле\", \"о том, что\", \"к тому же\", \"перед тем как\" и т.д,  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
